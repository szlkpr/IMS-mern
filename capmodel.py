# -*- coding: utf-8 -*-
"""Capmodel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kEV4c2ibaM02sfffhSQNW1Upz3ZZL8Ag
"""

# Install required packages
# !pip install torch torch-geometric networkx rdflib pykg2vec transformers scipy

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import networkx as nx
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from enum import Enum
import json
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# Knowledge Graph and Symbolic AI
from collections import defaultdict
import re
from transformers import pipeline, AutoTokenizer, AutoModel

# Scientific Computing
from scipy.integrate import solve_ivp
from scipy.optimize import minimize

class EntityType(Enum):
    """Types of entities in the knowledge graph"""
    PRODUCT = "product"
    FESTIVAL = "festival"
    WEATHER = "weather"
    SUPPLIER = "supplier"
    REGION = "region"
    POLICY = "policy"
    SEASON = "season"

class RelationType(Enum):
    """Types of relationships in the knowledge graph"""
    AFFECTS = "affects"
    DEPENDS_ON = "depends_on"
    CAUSES = "causes"
    CORRELATES_WITH = "correlates_with"
    SUPPLIES = "supplies"
    OCCURS_IN = "occurs_in"
    SIMILAR_TO = "similar_to"

@dataclass
class KnowledgeTriple:
    """Represents a knowledge triple (subject, predicate, object)"""
    subject: str
    predicate: RelationType
    object: str
    weight: float = 1.0
    confidence: float = 1.0
    temporal_window: Optional[Tuple[int, int]] = None  # Days before/after

class IndianMarketKnowledgeGraph:
    """
    Knowledge Graph specialized for Indian market dynamics
    Encodes domain expertise about festivals, weather, supply chains, etc.
    """

    def __init__(self):
        self.graph = nx.MultiDiGraph()
        self.entity_embeddings = {}
        self.relation_embeddings = {}
        self.rules = []  # Symbolic rules
        self.causal_graph = nx.DiGraph()

        # Initialize with domain knowledge
        self._build_base_knowledge()

    def _build_base_knowledge(self):
        """Build foundational knowledge about Indian markets"""

        festivals = [
            ("diwali", "onions", "affects", 3.0, (-7, 3)),  # Changed "increases_demand" to "affects"
            ("diwali", "sweets", "affects", 5.0, (-14, 7)),
            ("holi", "colors", "affects", 10.0, (-3, 1)),
            ("eid", "dates", "affects", 8.0, (-5, 2)),
            ("karva_chauth", "sargi_items", "affects", 4.0, (-2, 1)),
            ("onam", "vegetables", "affects", 2.5, (-5, 3)),
        ]

        # Weather knowledge
        weather_impacts = [
            ("monsoon", "rice", "affects", 2.0, (-30, 90)),  # Changed "affects_supply" to "affects"
            ("drought", "wheat", "affects", 4.0, (0, 180)),  # Changed "decreases_supply" to "affects"
            ("excessive_rain", "vegetables", "affects", 3.0, (0, 30)),
            ("winter", "woolen_goods", "affects", 2.0, (-15, 90)),  # Changed "increases_demand" to "affects"
            ("summer", "cooling_items", "affects", 3.0, (-10, 60)),
        ]

        # Seasonal patterns
        seasonal_knowledge = [
            ("winter_season", "heating_oil", "affects", 2.5, (-30, 120)),
            ("harvest_season", "storage_equipment", "affects", 2.0, (-15, 45)),
            ("school_season", "stationery", "affects", 3.0, (-7, 30)),
        ]

        # Supply chain knowledge
        supply_chain = [
            ("punjab", "wheat", "supplies", 1.0, None),  # Changed "major_supplier" to "supplies"
            ("kerala", "spices", "supplies", 1.0, None),
            ("maharashtra", "onions", "supplies", 1.0, None),
            ("west_bengal", "rice", "supplies", 1.0, None),
        ]

        # Policy impacts
        policy_impacts = [
            ("gst_rate_change", "all_products", "affects", 1.5, (0, 30)),  # Changed "affects_price" to "affects"
            ("export_ban", "commodities", "affects", 2.0, (0, 60)),  # Changed "increases_domestic_supply" to "affects"
            ("import_duty", "electronics", "affects", 1.8, (0, 90)),  # Changed "increases_price" to "affects"
            ("msp_announcement", "crops", "affects", 2.5, (0, 180)),  # Changed "sets_price_floor" to "affects"
        ]

        # Build knowledge triples
        all_knowledge = festivals + weather_impacts + seasonal_knowledge + supply_chain + policy_impacts

        for subject, obj, relation, weight, temporal in all_knowledge:
            triple = KnowledgeTriple(
                subject=subject,
                predicate=RelationType(relation.lower().replace("_", "_")),
                object=obj,
                weight=weight,
                confidence=0.9,
                temporal_window=temporal
            )
            self.add_triple(triple)

    def add_triple(self, triple: KnowledgeTriple):
        """Add a knowledge triple to the graph"""
        # Add nodes if they don't exist
        if not self.graph.has_node(triple.subject):
            self.graph.add_node(triple.subject, entity_type=self._infer_entity_type(triple.subject))

        if not self.graph.has_node(triple.object):
            self.graph.add_node(triple.object, entity_type=self._infer_entity_type(triple.object))

        # Add edge with attributes
        self.graph.add_edge(
            triple.subject,
            triple.object,
            relation=triple.predicate,
            weight=triple.weight,
            confidence=triple.confidence,
            temporal_window=triple.temporal_window
        )

        # Add to causal graph if it's a causal relationship
        if triple.predicate in [RelationType.CAUSES, RelationType.AFFECTS]:
            self.causal_graph.add_edge(triple.subject, triple.object, weight=triple.weight)

    def _infer_entity_type(self, entity: str) -> EntityType:
        """Infer entity type from entity name"""
        entity_lower = entity.lower()

        if any(festival in entity_lower for festival in ['diwali', 'holi', 'eid', 'onam', 'karva']):
            return EntityType.FESTIVAL
        elif any(weather in entity_lower for weather in ['monsoon', 'drought', 'rain', 'winter', 'summer']):
            return EntityType.WEATHER
        elif any(region in entity_lower for region in ['punjab', 'kerala', 'maharashtra', 'bengal']):
            return EntityType.REGION
        elif any(policy in entity_lower for policy in ['gst', 'export', 'import', 'msp']):
            return EntityType.POLICY
        elif any(season in entity_lower for season in ['season', 'harvest']):
            return EntityType.SEASON
        else:
            return EntityType.PRODUCT

    def get_related_entities(self, entity: str, max_hops: int = 2) -> List[Tuple[str, float]]:
        """Get entities related to given entity within max_hops"""
        if entity not in self.graph:
            return []

        related = []
        visited = set()

        def dfs(current_entity, current_weight, hops_remaining):
            if hops_remaining == 0 or current_entity in visited:
                return

            visited.add(current_entity)

            for neighbor in self.graph.neighbors(current_entity):
                edge_data = self.graph[current_entity][neighbor][0]  # Get first edge
                combined_weight = current_weight * edge_data.get('weight', 1.0)
                related.append((neighbor, combined_weight))

                if hops_remaining > 1:
                    dfs(neighbor, combined_weight, hops_remaining - 1)

        dfs(entity, 1.0, max_hops)

        # Sort by weight and remove duplicates
        unique_related = {}
        for ent, weight in related:
            if ent != entity:
                if ent not in unique_related or unique_related[ent] < weight:
                    unique_related[ent] = weight

        return sorted(unique_related.items(), key=lambda x: x[1], reverse=True)

    def extract_symbolic_rules(self, entity: str) -> List[str]:
        """Extract human-readable rules for an entity"""
        rules = []

        if entity not in self.graph:
            return rules

        # Direct impacts
        for neighbor in self.graph.neighbors(entity):
            edge_data = self.graph[entity][neighbor][0]
            relation = edge_data.get('relation')
            weight = edge_data.get('weight', 1.0)
            temporal = edge_data.get('temporal_window')

            if relation == RelationType.AFFECTS:
                effect = "increases" if weight > 1.0 else "decreases"
                rule = f"IF {entity.replace('_', ' ').title()} occurs, THEN {neighbor} demand {effect}"
                if temporal:
                    rule += f" (effect lasts {temporal[0]} to {temporal[1]} days)"
                rules.append(rule)

        # Causal chains
        if entity in self.causal_graph:
            for target in self.causal_graph.neighbors(entity):
                path_weight = self.causal_graph[entity][target].get('weight', 1.0)
                rule = f"IF {entity.replace('_', ' ').title()} changes, THEN {target} changes by factor {path_weight:.2f}"
                rules.append(rule)

        return rules

    def get_temporal_impact(self, source_entity: str, target_entity: str, days_offset: int) -> float:
        """Get impact strength at specific time offset"""
        if not self.graph.has_edge(source_entity, target_entity):
            return 0.0

        edge_data = self.graph[source_entity][target_entity][0]
        temporal_window = edge_data.get('temporal_window')
        base_weight = edge_data.get('weight', 1.0)

        if temporal_window is None:
            return base_weight

        start_day, end_day = temporal_window

        # Impact is strongest at the beginning and fades over time
        if start_day <= days_offset <= end_day:
            # Exponential decay
            decay_rate = 0.1
            time_in_window = (days_offset - start_day) / (end_day - start_day + 1)
            impact = base_weight * np.exp(-decay_rate * time_in_window)
            return impact
        else:
            return 0.0

    def visualize_subgraph(self, entity: str, max_hops: int = 1):
        """Visualize knowledge subgraph around an entity"""
        try:
            import matplotlib.pyplot as plt

            # Get subgraph
            related_entities = self.get_related_entities(entity, max_hops)
            nodes = [entity] + [ent for ent, _ in related_entities[:10]]  # Limit for visibility
            subgraph = self.graph.subgraph(nodes)

            # Create layout
            pos = nx.spring_layout(subgraph, k=1, iterations=50)

            # Draw graph
            plt.figure(figsize=(12, 8))

            # Color nodes by entity type
            entity_colors = {
                EntityType.PRODUCT: 'lightblue',
                EntityType.FESTIVAL: 'orange',
                EntityType.WEATHER: 'lightgreen',
                EntityType.REGION: 'pink',
                EntityType.POLICY: 'yellow',
                EntityType.SEASON: 'purple'
            }

            node_colors = [entity_colors.get(subgraph.nodes[node].get('entity_type'), 'gray')
                          for node in subgraph.nodes()]

            nx.draw(subgraph, pos,
                   node_color=node_colors,
                   node_size=1000,
                   font_size=8,
                   font_weight='bold',
                   arrows=True,
                   edge_color='gray',
                   alpha=0.7)

            # Add labels
            nx.draw_networkx_labels(subgraph, pos, font_size=8)

            plt.title(f"Knowledge Graph for: {entity.replace('_', ' ').title()}")
            plt.axis('off')
            plt.tight_layout()
            plt.show()

        except ImportError:
            print("Matplotlib not available. Install it to visualize knowledge graphs.")

    def get_knowledge_summary(self) -> Dict[str, Any]:
        """Get summary statistics of the knowledge graph"""
        return {
            'total_entities': self.graph.number_of_nodes(),
            'total_relationships': self.graph.number_of_edges(),
            'entity_types': {et.value: len([n for n in self.graph.nodes()
                                          if self.graph.nodes[n].get('entity_type') == et])
                           for et in EntityType},
            'most_connected_entities': sorted([(node, self.graph.degree(node))
                                             for node in self.graph.nodes()],
                                            key=lambda x: x[1], reverse=True)[:5],
            'causal_relationships': self.causal_graph.number_of_edges()
        }

# Example usage and testing
def test_knowledge_graph():
    """Test the knowledge graph functionality"""
    kg = IndianMarketKnowledgeGraph()

    print("=== Knowledge Graph Summary ===")
    summary = kg.get_knowledge_summary()
    for key, value in summary.items():
        print(f"{key}: {value}")

    print("\n=== Testing Entity Relationships ===")
    test_entity = "onions"
    related = kg.get_related_entities(test_entity, max_hops=2)
    print(f"Entities related to '{test_entity}':")
    for entity, weight in related[:5]:
        print(f"  {entity}: {weight:.2f}")

    print(f"\n=== Symbolic Rules for '{test_entity}' ===")
    rules = kg.extract_symbolic_rules(test_entity)
    for i, rule in enumerate(rules, 1):
        print(f"{i}. {rule}")

    print(f"\n=== Temporal Impact Analysis ===")
    print("Impact of Diwali on onions over time:")
    for day_offset in [-10, -5, 0, 5, 10]:
        impact = kg.get_temporal_impact("diwali", "onions", day_offset)
        print(f"Day {day_offset:+3d}: {impact:.3f}")

    return kg

if __name__ == "__main__":
    kg = test_knowledge_graph()

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Tuple, Callable
from dataclasses import dataclass
from scipy.integrate import solve_ivp
import matplotlib.pyplot as plt

@dataclass
class MarketState:
    """Represents the state of a market at any given time"""
    price: float
    supply: float
    demand: float
    inventory: float
    momentum: float  # Price momentum
    volatility: float  # Market volatility
    timestamp: datetime

class PhysicsInformedMarketDynamics(nn.Module):
    """
    Physics-Informed Neural Network for market dynamics
    Models market behavior using differential equations analogous to physical systems
    """

    def __init__(self, state_dim: int = 6, hidden_dim: int = 64):
        super().__init__()

        # Neural network layers for learning market physics
        self.state_dim = state_dim  # [price, supply, demand, inventory, momentum, volatility]
        self.hidden_dim = hidden_dim

        # Physics-informed layers
        self.supply_demand_net = nn.Sequential(
            nn.Linear(state_dim + 4, hidden_dim),  # +4 for external factors
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 2)  # Output: supply_rate, demand_rate
        )

        self.price_dynamics_net = nn.Sequential(
            nn.Linear(state_dim + 2, hidden_dim),  # +2 for supply/demand rates
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1)  # Output: price_rate
        )

        self.inventory_flow_net = nn.Sequential(
            nn.Linear(state_dim + 3, hidden_dim),  # +3 for rates
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1)  # Output: inventory_rate
        )

        self.momentum_volatility_net = nn.Sequential(
            nn.Linear(state_dim + 1, hidden_dim),  # +1 for price_rate
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 2)  # Output: momentum_rate, volatility_rate
        )

        # Physics constraints
        self.conservation_weight = 1.0
        self.equilibrium_weight = 0.5
        self.stability_weight = 0.3

    def market_ode(self, t: float, state: np.ndarray, external_factors: Dict[str, float]) -> np.ndarray:
        """
        Ordinary Differential Equation governing market dynamics
        Based on physics principles:
        - Conservation of mass (supply + demand = market flow)
        - Energy conservation (price momentum)
        - Diffusion equations (price propagation)
        - Thermodynamics (volatility as entropy)
        """

        # Convert to torch tensors
        state_tensor = torch.FloatTensor(state).unsqueeze(0)

        # External factors: [festival_impact, weather_impact, policy_impact, seasonal_factor]
        ext_factors = torch.FloatTensor([
            external_factors.get('festival', 0.0),
            external_factors.get('weather', 0.0),
            external_factors.get('policy', 0.0),
            external_factors.get('seasonal', 0.0)
        ]).unsqueeze(0)

        # Predict supply and demand rates
        supply_demand_input = torch.cat([state_tensor, ext_factors], dim=1)
        supply_rate, demand_rate = self.supply_demand_net(supply_demand_input)[0]

        # Predict price dynamics
        price_input = torch.cat([state_tensor, torch.tensor([[supply_rate, demand_rate]])], dim=1)
        price_rate = self.price_dynamics_net(price_input)[0, 0]

        # Predict inventory flow
        inventory_input = torch.cat([state_tensor, torch.tensor([[supply_rate, demand_rate, price_rate]])], dim=1)
        inventory_rate = self.inventory_flow_net(inventory_input)[0, 0]

        # Predict momentum and volatility
        momentum_vol_input = torch.cat([state_tensor, torch.tensor([[price_rate]])], dim=1)
        momentum_rate, volatility_rate = self.momentum_volatility_net(momentum_vol_input)[0]

        # Assemble derivatives
        dstate_dt = np.array([
            price_rate.item(),
            supply_rate.item(),
            demand_rate.item(),
            inventory_rate.item(),
            momentum_rate.item(),
            volatility_rate.item()
        ])

        return dstate_dt

    def physics_loss(self, predicted_states: torch.Tensor, external_factors: torch.Tensor) -> torch.Tensor:
        """
        Compute physics-informed loss based on conservation laws and market principles
        """
        batch_size, seq_len, state_dim = predicted_states.shape
        total_loss = 0.0

        # Extract state components
        prices = predicted_states[:, :, 0]
        supplies = predicted_states[:, :, 1]
        demands = predicted_states[:, :, 2]
        inventories = predicted_states[:, :, 3]
        momenta = predicted_states[:, :, 4]
        volatilities = predicted_states[:, :, 5]

        # 1. Conservation Law: Supply + Demand changes should balance
        supply_changes = torch.diff(supplies, dim=1)
        demand_changes = torch.diff(demands, dim=1)
        conservation_loss = torch.mean((supply_changes + demand_changes) ** 2)

        # 2. Market Equilibrium: Price should move towards supply-demand balance
        supply_demand_imbalance = supplies - demands
        price_changes = torch.diff(prices, dim=1)
        equilibrium_loss = torch.mean((price_changes - 0.1 * supply_demand_imbalance[:, :-1]) ** 2)

        # 3. Inventory Flow: Inventory changes should follow supply-demand
        inventory_changes = torch.diff(inventories, dim=1)
        expected_inventory_change = supply_changes - demand_changes
        inventory_loss = torch.mean((inventory_changes - expected_inventory_change) ** 2)

        # 4. Momentum Conservation: Price momentum should be continuous
        momentum_changes = torch.diff(momenta, dim=1)
        momentum_loss = torch.mean(momentum_changes ** 2)

        # 5. Volatility Bounds: Volatility should be non-negative and bounded
        volatility_loss = torch.mean(torch.relu(-volatilities) + torch.relu(volatilities - 10))

        # 6. Energy Conservation: Total market "energy" should be conserved
        market_energy = 0.5 * (prices ** 2 + momenta ** 2) + volatilities
        energy_changes = torch.diff(market_energy, dim=1)
        energy_loss = torch.mean(energy_changes ** 2)

        # Combine losses
        total_loss = (
            self.conservation_weight * conservation_loss +
            self.equilibrium_weight * equilibrium_loss +
            0.3 * inventory_loss +
            0.2 * momentum_loss +
            0.1 * volatility_loss +
            0.2 * energy_loss
        )

        return total_loss

    def solve_market_dynamics(self, initial_state: MarketState,
                            external_factors_func: Callable[[float], Dict[str, float]],
                            time_horizon: int = 30) -> List[MarketState]:
        """
        Solve market dynamics over time using physics-informed ODEs
        """

        # Initial conditions
        y0 = np.array([
            initial_state.price,
            initial_state.supply,
            initial_state.demand,
            initial_state.inventory,
            initial_state.momentum,
            initial_state.volatility
        ])

        # Time span
        t_span = (0, time_horizon)
        t_eval = np.arange(0, time_horizon + 1)

        # Solve ODE with varying external factors
        def ode_func(t, y):
            ext_factors = external_factors_func(t)
            return self.market_ode(t, y, ext_factors)

        # Solve
        solution = solve_ivp(ode_func, t_span, y0, t_eval=t_eval, method='RK45')

        # Convert solution to MarketState objects
        states = []
        for i, t in enumerate(solution.t):
            state = MarketState(
                price=float(solution.y[0, i]),
                supply=float(solution.y[1, i]),
                demand=float(solution.y[2, i]),
                inventory=float(solution.y[3, i]),
                momentum=float(solution.y[4, i]),
                volatility=float(solution.y[5, i]),
                timestamp=initial_state.timestamp + timedelta(days=int(t))
            )
            states.append(state)

        return states

class MarketPhysicsEngine:
    """
    High-level engine that combines physics-informed dynamics with knowledge graph
    """

    def __init__(self, knowledge_graph: IndianMarketKnowledgeGraph):
        self.kg = knowledge_graph
        self.physics_model = PhysicsInformedMarketDynamics()
        self.trained = False

    def get_external_factors(self, product: str, timestamp: datetime) -> Dict[str, float]:
        """
        Extract external factors from knowledge graph for given product and time
        """
        factors = {
            'festival': 0.0,
            'weather': 0.0,
            'policy': 0.0,
            'seasonal': 0.0
        }

        # Get related entities from knowledge graph
        related_entities = self.kg.get_related_entities(product, max_hops=2)

        for entity, weight in related_entities:
            entity_type = self.kg.graph.nodes[entity].get('entity_type')

            # Calculate time-based impact
            days_since_ref = 0  # This would be calculated based on actual dates
            temporal_impact = self.kg.get_temporal_impact(entity, product, days_since_ref)

            if entity_type == EntityType.FESTIVAL:
                factors['festival'] += weight * temporal_impact
            elif entity_type == EntityType.WEATHER:
                factors['weather'] += weight * temporal_impact
            elif entity_type == EntityType.POLICY:
                factors['policy'] += weight * temporal_impact
            elif entity_type == EntityType.SEASON:
                factors['seasonal'] += weight * temporal_impact

        # Normalize factors to reasonable ranges
        for key in factors:
            factors[key] = np.tanh(factors[key])  # Keep between -1 and 1

        return factors

    def train_physics_model(self, historical_data: Dict[str, pd.DataFrame], epochs: int = 100):
        """
        Train the physics-informed model on historical market data
        """
        print("Training Physics-Informed Market Model...")

        optimizer = torch.optim.Adam(self.physics_model.parameters(), lr=0.001)
        losses = []

        for epoch in range(epochs):
            epoch_loss = 0.0

            for product, df in historical_data.items():
                if len(df) < 10:
                    continue

                # Prepare data
                states = self._prepare_training_data(df, product)
                if len(states) < 5:
                    continue

                # Convert to tensors
                state_tensor = torch.FloatTensor(states).unsqueeze(0)  # [1, seq_len, state_dim]

                # Get external factors for each time step
                ext_factors_list = []
                for i, row in df.iterrows():
                    timestamp = pd.to_datetime(row['date']) if 'date' in row else datetime.now()
                    factors = self.get_external_factors(product, timestamp)
                    ext_factors_list.append(list(factors.values()))

                ext_factors_tensor = torch.FloatTensor(ext_factors_list).unsqueeze(0)

                # Forward pass through neural networks (simulate dynamics)
                predicted_states = self._simulate_dynamics(state_tensor[:, :-1], ext_factors_tensor[:, :-1])

                # Compute losses
                mse_loss = nn.MSELoss()(predicted_states, state_tensor[:, 1:])
                physics_loss = self.physics_model.physics_loss(state_tensor, ext_factors_tensor)

                total_loss = mse_loss + 0.1 * physics_loss

                # Backward pass
                optimizer.zero_grad()
                total_loss.backward()
                optimizer.step()

                epoch_loss += total_loss.item()

            losses.append(epoch_loss)

            if epoch % 20 == 0:
                print(f"Epoch {epoch}, Loss: {epoch_loss:.4f}")

        self.trained = True
        print("Physics model training completed!")
        return losses

    def _prepare_training_data(self, df: pd.DataFrame, product: str) -> List[List[float]]:
        """
        Convert DataFrame to state vectors for training
        """
        states = []

        for i, row in df.iterrows():
            # Calculate derived features if not present
            price = row.get('price', row.get('value', 0))

            # Estimate supply/demand from price changes and external factors
            if i > 0:
                price_change = price - df.iloc[i-1].get('price', df.iloc[i-1].get('value', price))
                supply = max(0, 100 - price_change * 10)  # Inverse relationship
                demand = max(0, 100 + price_change * 10)  # Direct relationship
            else:
                supply = 100.0
                demand = 100.0

            # Estimate inventory from moving average
            window_size = min(7, i + 1)
            recent_prices = df.iloc[max(0, i-window_size+1):i+1].get('price',
                                   df.iloc[max(0, i-window_size+1):i+1].get('value', [price]))
            if hasattr(recent_prices, 'mean'):
                avg_price = recent_prices.mean()
            else:
                avg_price = price

            inventory = max(0, 200 - abs(price - avg_price) * 5)

            # Calculate momentum (price velocity)
            if i > 0:
                momentum = price_change
            else:
                momentum = 0.0

            # Calculate volatility (rolling standard deviation)
            if i >= 3:
                recent_prices = df.iloc[max(0, i-6):i+1].get('price',
                               df.iloc[max(0, i-6):i+1].get('value', [price]))
                if hasattr(recent_prices, 'std'):
                    volatility = recent_prices.std()
                else:
                    volatility = 1.0
            else:
                volatility = 1.0

            state = [price, supply, demand, inventory, momentum, volatility]
            states.append(state)

        return states

    def _simulate_dynamics(self, initial_states: torch.Tensor,
                          external_factors: torch.Tensor) -> torch.Tensor:
        """
        Simulate market dynamics using neural networks
        """
        batch_size, seq_len, state_dim = initial_states.shape
        predicted_states = []

        current_state = initial_states[:, 0, :]  # Start with first state

        for t in range(seq_len):
            # Get external factors for this timestep
            ext_factors = external_factors[:, t, :]

            # Predict next state using physics-informed networks
            supply_demand_input = torch.cat([current_state, ext_factors], dim=1)
            supply_rate, demand_rate = self.physics_model.supply_demand_net(supply_demand_input).unbind(dim=1)

            price_input = torch.cat([current_state, supply_rate.unsqueeze(1), demand_rate.unsqueeze(1)], dim=1)
            price_rate = self.physics_model.price_dynamics_net(price_input).squeeze(1)

            inventory_input = torch.cat([current_state, supply_rate.unsqueeze(1),
                                       demand_rate.unsqueeze(1), price_rate.unsqueeze(1)], dim=1)
            inventory_rate = self.physics_model.inventory_flow_net(inventory_input).squeeze(1)

            momentum_vol_input = torch.cat([current_state, price_rate.unsqueeze(1)], dim=1)
            momentum_rate, volatility_rate = self.physics_model.momentum_volatility_net(momentum_vol_input).unbind(dim=1)

            # Update state (Euler integration with dt=1)
            dt = 1.0
            next_state = current_state + dt * torch.stack([
                price_rate, supply_rate, demand_rate,
                inventory_rate, momentum_rate, volatility_rate
            ], dim=1)

            predicted_states.append(next_state.unsqueeze(1))
            current_state = next_state

        return torch.cat(predicted_states, dim=1)

    def forecast_market_dynamics(self, product: str, initial_state: MarketState,
                               forecast_horizon: int = 30) -> List[MarketState]:
        """
        Forecast market dynamics using physics-informed model
        """
        if not self.trained:
            raise ValueError("Model must be trained first!")

        def external_factors_func(t: float) -> Dict[str, float]:
            future_timestamp = initial_state.timestamp + timedelta(days=int(t))
            return self.get_external_factors(product, future_timestamp)

        # Use the physics-informed ODE solver
        forecasted_states = self.physics_model.solve_market_dynamics(
            initial_state, external_factors_func, forecast_horizon
        )

        return forecasted_states

    def analyze_market_stability(self, product: str, states: List[MarketState]) -> Dict[str, Any]:
        """
        Analyze market stability using physics principles
        """
        if len(states) < 2:
            return {"stability_score": 0.5, "analysis": "Insufficient data"}

        prices = [s.price for s in states]
        supplies = [s.supply for s in states]
        demands = [s.demand for s in states]
        volatilities = [s.volatility for s in states]

        # Calculate stability metrics
        price_stability = 1 / (1 + np.std(prices) / np.mean(prices))
        supply_demand_balance = 1 - np.mean([abs(s - d) / (s + d + 1e-6)
                                           for s, d in zip(supplies, demands)])
        volatility_trend = -np.corrcoef(range(len(volatilities)), volatilities)[0, 1]

        # Energy conservation check
        energies = [0.5 * (s.price ** 2 + s.momentum ** 2) + s.volatility for s in states]
        energy_conservation = 1 / (1 + np.std(energies) / np.mean(energies))

        stability_score = (price_stability + supply_demand_balance +
                         max(0, volatility_trend) + energy_conservation) / 4

        analysis = {
            "stability_score": stability_score,
            "price_stability": price_stability,
            "supply_demand_balance": supply_demand_balance,
            "volatility_trend": "Decreasing" if volatility_trend > 0 else "Increasing",
            "energy_conservation": energy_conservation,
            "market_phase": self._classify_market_phase(states[-1])
        }

        return analysis

    def _classify_market_phase(self, state: MarketState) -> str:
        """
        Classify current market phase based on state
        """
        supply_demand_ratio = state.supply / (state.demand + 1e-6)

        if supply_demand_ratio > 1.2:
            return "Oversupply"
        elif supply_demand_ratio < 0.8:
            return "High Demand"
        elif state.volatility > 5:
            return "Volatile"
        elif abs(state.momentum) < 0.1:
            return "Stable"
        else:
            return "Trending"

    def visualize_market_dynamics(self, states: List[MarketState], title: str = "Market Dynamics"):
        """
        Visualize market dynamics over time
        """
        try:
            timestamps = [s.timestamp for s in states]
            prices = [s.price for s in states]
            supplies = [s.supply for s in states]
            demands = [s.demand for s in states]
            inventories = [s.inventory for s in states]
            volatilities = [s.volatility for s in states]

            fig, axes = plt.subplots(2, 2, figsize=(15, 10))

            # Price and momentum
            axes[0, 0].plot(timestamps, prices, 'b-', label='Price', linewidth=2)
            axes[0, 0].set_title('Price Evolution')
            axes[0, 0].set_ylabel('Price')
            axes[0, 0].legend()
            axes[0, 0].grid(True)

            # Supply and demand
            axes[0, 1].plot(timestamps, supplies, 'g-', label='Supply', linewidth=2)
            axes[0, 1].plot(timestamps, demands, 'r-', label='Demand', linewidth=2)
            axes[0, 1].set_title('Supply vs Demand')
            axes[0, 1].set_ylabel('Quantity')
            axes[0, 1].legend()
            axes[0, 1].grid(True)

            # Inventory
            axes[1, 0].plot(timestamps, inventories, 'm-', label='Inventory', linewidth=2)
            axes[1, 0].set_title('Inventory Levels')
            axes[1, 0].set_ylabel('Inventory')
            axes[1, 0].legend()
            axes[1, 0].grid(True)

            # Volatility
            axes[1, 1].plot(timestamps, volatilities, 'orange', label='Volatility', linewidth=2)
            axes[1, 1].set_title('Market Volatility')
            axes[1, 1].set_ylabel('Volatility')
            axes[1, 1].legend()
            axes[1, 1].grid(True)

            plt.suptitle(title, fontsize=16)
            plt.tight_layout()
            plt.show()

        except ImportError:
            print("Matplotlib not available. Install it to visualize market dynamics.")

# Example usage and testing
def test_physics_engine():
    """Test the physics-informed market engine"""

    # Initialize with knowledge graph
    from part1_knowledge_graph import IndianMarketKnowledgeGraph
    kg = IndianMarketKnowledgeGraph()
    engine = MarketPhysicsEngine(kg)

    print("=== Testing Physics-Informed Market Engine ===")

    # Create sample historical data
    dates = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')
    sample_data = {
        'onions': pd.DataFrame({
            'date': dates,
            'price': 50 + 10 * np.sin(2 * np.pi * np.arange(len(dates)) / 365) +
                    np.random.normal(0, 5, len(dates))
        })
    }

    # Train the model
    losses = engine.train_physics_model(sample_data, epochs=50)
    print(f"Training completed. Final loss: {losses[-1]:.4f}")

    # Test forecasting
    initial_state = MarketState(
        price=55.0,
        supply=100.0,
        demand=95.0,
        inventory=200.0,
        momentum=0.5,
        volatility=2.0,
        timestamp=datetime(2024, 1, 1)
    )

    print("\n=== Forecasting Market Dynamics ===")
    forecast = engine.forecast_market_dynamics('onions', initial_state, forecast_horizon=30)

    print(f"Forecast generated for {len(forecast)} time steps")
    print(f"Initial price: {forecast[0].price:.2f}")
    print(f"Final price: {forecast[-1].price:.2f}")
    print(f"Price change: {((forecast[-1].price / forecast[0].price) - 1) * 100:.1f}%")

    # Analyze stability
    stability_analysis = engine.analyze_market_stability('onions', forecast)
    print(f"\n=== Market Stability Analysis ===")
    for key, value in stability_analysis.items():
        print(f"{key}: {value}")

    return engine, forecast

if __name__ == "__main__":
    engine, forecast = test_physics_engine()

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Any, Optional, Set
from dataclasses import dataclass
from enum import Enum
import re
import json
from datetime import datetime, timedelta
from collections import defaultdict

class RuleType(Enum):
    """Types of symbolic rules"""
    CONDITIONAL = "conditional"  # IF-THEN rules
    CAUSAL = "causal"           # X CAUSES Y
    TEMPORAL = "temporal"       # X happens BEFORE/AFTER Y
    LOGICAL = "logical"         # AND, OR, NOT operations
    CONSTRAINT = "constraint"   # Mathematical constraints

class ConfidenceLevel(Enum):
    """Confidence levels for symbolic rules"""
    VERY_HIGH = 0.9
    HIGH = 0.8
    MEDIUM = 0.6
    LOW = 0.4
    VERY_LOW = 0.2

@dataclass
class SymbolicRule:
    """Represents a symbolic rule in the reasoning system"""
    rule_id: str
    rule_type: RuleType
    condition: str  # Natural language condition
    conclusion: str  # Natural language conclusion
    confidence: float
    temporal_scope: Optional[Tuple[int, int]] = None  # (start_days, end_days)
    strength: float = 1.0  # Rule strength/weight
    context: Dict[str, Any] = None  # Additional context

    def __post_init__(self):
        if self.context is None:
            self.context = {}

class SymbolicKnowledgeBase:
    """
    Knowledge base storing symbolic rules and logical relationships
    """

    def __init__(self):
        self.rules: Dict[str, SymbolicRule] = {}
        self.rule_chains: Dict[str, List[str]] = defaultdict(list)
        self.fact_base: Dict[str, Any] = {}
        self.inference_cache: Dict[str, Any] = {}

    def add_rule(self, rule: SymbolicRule):
        """Add a symbolic rule to the knowledge base"""
        self.rules[rule.rule_id] = rule

        # Extract entities from rule for chain building
        entities = self._extract_entities(rule.condition + " " + rule.conclusion)
        for entity in entities:
            self.rule_chains[entity].append(rule.rule_id)

    def _extract_entities(self, text: str) -> Set[str]:
        """Extract entities from rule text"""
        # Simple entity extraction (could be enhanced with NER)
        entities = set()

        # Common market entities patterns
        patterns = [
            r'\b(price|demand|supply|inventory|cost)\s+of\s+(\w+)',
            r'\b(\w+)\s+(price|demand|supply|inventory)',
            r'\b(diwali|holi|eid|monsoon|winter|summer)\b',
            r'\b(\w+)\s+market\b',
        ]

        text_lower = text.lower()
        for pattern in patterns:
            matches = re.findall(pattern, text_lower)
            for match in matches:
                if isinstance(match, tuple):
                    entities.update(match)
                else:
                    entities.add(match)

        return {e for e in entities if len(e) > 2}

    def get_applicable_rules(self, context: Dict[str, Any]) -> List[SymbolicRule]:
        """Get rules applicable to current context"""
        applicable = []

        for rule in self.rules.values():
            if self._is_rule_applicable(rule, context):
                applicable.append(rule)

        return sorted(applicable, key=lambda r: r.confidence, reverse=True)

    def _is_rule_applicable(self, rule: SymbolicRule, context: Dict[str, Any]) -> bool:
        """Check if a rule is applicable in current context"""
        # Check temporal constraints
        if rule.temporal_scope and 'days_offset' in context:
            start_day, end_day = rule.temporal_scope
            days_offset = context['days_offset']
            if not (start_day <= days_offset <= end_day):
                return False

        # Check context requirements
        if 'required_entities' in rule.context:
            required = rule.context['required_entities']
            available = set(context.get('entities', []))
            if not set(required).issubset(available):
                return False

        return True

    def chain_rules(self, start_entity: str, max_depth: int = 3) -> List[List[str]]:
        """Find rule chains starting from an entity"""
        chains = []

        def dfs_rules(current_entity: str, current_chain: List[str], depth: int):
            if depth >= max_depth:
                return

            for rule_id in self.rule_chains.get(current_entity, []):
                rule = self.rules[rule_id]

                # Avoid cycles
                if rule_id in current_chain:
                    continue

                new_chain = current_chain + [rule_id]
                chains.append(new_chain.copy())

                # Find next entities in the conclusion
                conclusion_entities = self._extract_entities(rule.conclusion)
                for next_entity in conclusion_entities:
                    if next_entity != current_entity:
                        dfs_rules(next_entity, new_chain, depth + 1)

        dfs_rules(start_entity, [], 0)
        return chains

class LogicalReasoner:
    """
    Performs logical reasoning using symbolic rules
    """

    def __init__(self, knowledge_base: SymbolicKnowledgeBase):
        self.kb = knowledge_base
        self.inference_steps = []

    def infer(self, query: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Perform logical inference to answer a query
        """
        self.inference_steps = []
        result = {
            'answer': None,
            'confidence': 0.0,
            'reasoning_chain': [],
            'applicable_rules': []
        }

        # Get applicable rules
        applicable_rules = self.kb.get_applicable_rules(context)
        result['applicable_rules'] = [r.rule_id for r in applicable_rules]

        # Perform forward chaining
        inferred_facts = self._forward_chaining(applicable_rules, context)

        # Try to answer the query
        answer, confidence = self._match_query(query, inferred_facts)

        result['answer'] = answer
        result['confidence'] = confidence
        result['reasoning_chain'] = self.inference_steps.copy()

        return result

    def _forward_chaining(self, rules: List[SymbolicRule], context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Forward chaining inference
        """
        facts = context.copy()
        iterations = 0
        max_iterations = 10

        while iterations < max_iterations:
            new_facts = {}
            facts_added = False

            for rule in rules:
                if self._evaluate_condition(rule.condition, facts):
                    conclusion_facts = self._extract_conclusion_facts(rule.conclusion, rule)

                    for fact_key, fact_value in conclusion_facts.items():
                        if fact_key not in facts:
                            new_facts[fact_key] = fact_value
                            facts_added = True

                            # Record inference step
                            self.inference_steps.append({
                                'rule_id': rule.rule_id,
                                'condition': rule.condition,
                                'conclusion': rule.conclusion,
                                'inferred_fact': {fact_key: fact_value},
                                'confidence': rule.confidence
                            })

            facts.update(new_facts)

            if not facts_added:
                break

            iterations += 1

        return facts

    def _evaluate_condition(self, condition: str, facts: Dict[str, Any]) -> bool:
        """
        Evaluate if a condition is satisfied by current facts
        """
        condition_lower = condition.lower()

        # Simple condition evaluation patterns
        if "if" in condition_lower:
            # Extract condition part after "if"
            if_part = condition_lower.split("if", 1)[1].strip()

            # Check for entity presence
            for entity in facts.get('entities', []):
                if entity in if_part:
                    return True

            # Check for specific conditions
            if "festival" in if_part and facts.get('festival_active', False):
                return True

            if "high demand" in if_part and facts.get('demand_level', 'normal') == 'high':
                return True

            if "low supply" in if_part and facts.get('supply_level', 'normal') == 'low':
                return True

        return False

    def _extract_conclusion_facts(self, conclusion: str, rule: SymbolicRule) -> Dict[str, Any]:
        """
        Extract facts from rule conclusion
        """
        facts = {}
        conclusion_lower = conclusion.lower()

        # Extract price changes
        if "price" in conclusion_lower:
            if "increases" in conclusion_lower or "rises" in conclusion_lower:
                facts['price_direction'] = 'increase'
                facts['price_change_factor'] = rule.strength
            elif "decreases" in conclusion_lower or "falls" in conclusion_lower:
                facts['price_direction'] = 'decrease'
                facts['price_change_factor'] = rule.strength

        # Extract demand changes
        if "demand" in conclusion_lower:
            if "increases" in conclusion_lower or "rises" in conclusion_lower:
                facts['demand_direction'] = 'increase'
                facts['demand_change_factor'] = rule.strength
            elif "decreases" in conclusion_lower or "falls" in conclusion_lower:
                facts['demand_direction'] = 'decrease'
                facts['demand_change_factor'] = rule.strength

        # Extract supply changes
        if "supply" in conclusion_lower:
            if "increases" in conclusion_lower or "rises" in conclusion_lower:
                facts['supply_direction'] = 'increase'
                facts['supply_change_factor'] = rule.strength
            elif "decreases" in conclusion_lower or "falls" in conclusion_lower:
                facts['supply_direction'] = 'decrease'
                facts['supply_change_factor'] = rule.strength

        return facts

    def _match_query(self, query: str, facts: Dict[str, Any]) -> Tuple[Any, float]:
        """
        Match query against inferred facts
        """
        query_lower = query.lower()

        if "price" in query_lower:
            if 'price_direction' in facts:
                direction = facts['price_direction']
                factor = facts.get('price_change_factor', 1.0)
                confidence = min(0.9, factor * 0.8)
                return f"Price will {direction}", confidence

        if "demand" in query_lower:
            if 'demand_direction' in facts:
                direction = facts['demand_direction']
                factor = facts.get('demand_change_factor', 1.0)
                confidence = min(0.9, factor * 0.8)
                return f"Demand will {direction}", confidence

        return "Cannot determine", 0.0

class CausalInferenceEngine:
    """
    Performs causal inference to understand cause-effect relationships
    """

    def __init__(self, knowledge_graph):
        self.kg = knowledge_graph
        self.causal_graph = knowledge_graph.causal_graph.copy()

    def discover_causal_relationships(self, data: pd.DataFrame, variables: List[str]) -> Dict[str, List[str]]:
        """
        Discover causal relationships from data using various methods
        """
        causal_relationships = defaultdict(list)

        # Method 1: Temporal precedence
        temporal_causality = self._temporal_causal_discovery(data, variables)
        for cause, effects in temporal_causality.items():
            causal_relationships[cause].extend(effects)

        # Method 2: Granger causality (simplified)
        granger_causality = self._granger_causality(data, variables)
        for cause, effects in granger_causality.items():
            causal_relationships[cause].extend(effects)

        # Method 3: Knowledge-based causality
        knowledge_causality = self._knowledge_based_causality(variables)
        for cause, effects in knowledge_causality.items():
            causal_relationships[cause].extend(effects)

        # Remove duplicates
        for cause in causal_relationships:
            causal_relationships[cause] = list(set(causal_relationships[cause]))

        return dict(causal_relationships)

    def _temporal_causal_discovery(self, data: pd.DataFrame, variables: List[str]) -> Dict[str, List[str]]:
        """
        Discover causality based on temporal precedence
        """
        causality = defaultdict(list)

        for var1 in variables:
            for var2 in variables:
                if var1 == var2:
                    continue

                if var1 in data.columns and var2 in data.columns:
                    # Check if changes in var1 precede changes in var2
                    correlation = self._lagged_correlation(data[var1], data[var2])
                    if correlation > 0.3:  # Threshold for causality
                        causality[var1].append(var2)

        return dict(causality)

    def _lagged_correlation(self, series1: pd.Series, series2: pd.Series, max_lag: int = 5) -> float:
        """
        Calculate maximum lagged correlation
        """
        max_corr = 0.0

        for lag in range(1, min(max_lag + 1, len(series1) // 4)):
            if lag < len(series1):
                lagged_series1 = series1.shift(lag).dropna()
                aligned_series2 = series2.iloc[lag:].dropna()

                if len(lagged_series1) > 0 and len(aligned_series2) > 0:
                    min_len = min(len(lagged_series1), len(aligned_series2))
                    if min_len > 5:
                        corr = np.corrcoef(lagged_series1[:min_len], aligned_series2[:min_len])[0, 1]
                        if not np.isnan(corr):
                            max_corr = max(max_corr, abs(corr))

        return max_corr

    def _granger_causality(self, data: pd.DataFrame, variables: List[str]) -> Dict[str, List[str]]:
        """
        Simplified Granger causality test
        """
        causality = defaultdict(list)

        for var1 in variables:
            for var2 in variables:
                if var1 == var2 or var1 not in data.columns or var2 not in data.columns:
                    continue

                # Test if past values of var1 help predict var2
                try:
                    series1 = data[var1].dropna()
                    series2 = data[var2].dropna()

                    if len(series1) > 10 and len(series2) > 10:
                        # Simple lag regression
                        lag = 3
                        X = series1.shift(lag).dropna()
                        y = series2.iloc[lag:].dropna()

                        min_len = min(len(X), len(y))
                        if min_len > 10:
                            X = X[:min_len].values.reshape(-1, 1)
                            y = y[:min_len].values

                            # Calculate predictive power
                            corr = np.corrcoef(X.flatten(), y)[0, 1]
                            if abs(corr) > 0.25:
                                causality[var1].append(var2)
                except Exception as e:
                    continue

        return dict(causality)

    def _knowledge_based_causality(self, variables: List[str]) -> Dict[str, List[str]]:
        """
        Use knowledge graph to infer causal relationships
        """
        causality = defaultdict(list)

        for var in variables:
            if var in self.causal_graph:
                for target in self.causal_graph.neighbors(var):
                    if target in variables:
                        causality[var].append(target)

        return dict(causality)

    def counterfactual_analysis(self, scenario: Dict[str, Any],
                               baseline: Dict[str, Any]) -> Dict[str, Any]:
        """
        Perform counterfactual analysis: "What if X had been different?"
        """
        result = {
            'scenario': scenario,
            'baseline': baseline,
            'differences': {},
            'causal_effects': [],
            'confidence': 0.0
        }

        # Find differences between scenario and baseline
        for key in scenario:
            if key in baseline:
                if scenario[key] != baseline[key]:
                    result['differences'][key] = {
                        'scenario_value': scenario[key],
                        'baseline_value': baseline[key],
                        'change': scenario[key] - baseline[key] if isinstance(scenario[key], (int, float)) else 'changed'
                    }

        # Trace causal effects of differences
        for changed_var in result['differences']:
            if changed_var in self.causal_graph:
                # Find all affected variables
                affected_vars = self._find_causal_descendants(changed_var)

                for affected_var in affected_vars:
                    # Estimate effect magnitude
                    path_length = len(nx.shortest_path(self.causal_graph, changed_var, affected_var))
                    effect_strength = self._estimate_causal_effect(changed_var, affected_var)

                    result['causal_effects'].append({
                        'cause': changed_var,
                        'effect': affected_var,
                        'path_length': path_length,
                        'estimated_strength': effect_strength
                    })

        # Calculate overall confidence
        result['confidence'] = min(0.9, len(result['causal_effects']) * 0.15)

        return result

    def _find_causal_descendants(self, node: str, max_depth: int = 3) -> List[str]:
        """
        Find all nodes causally influenced by the given node
        """
        descendants = []
        visited = set()

        def dfs(current, depth):
            if depth >= max_depth or current in visited:
                return
            visited.add(current)

            if current in self.causal_graph:
                for neighbor in self.causal_graph.neighbors(current):
                    descendants.append(neighbor)
                    dfs(neighbor, depth + 1)

        dfs(node, 0)
        return list(set(descendants))

    def _estimate_causal_effect(self, cause: str, effect: str) -> float:
        """
        Estimate the strength of causal effect
        """
        if not self.causal_graph.has_edge(cause, effect):
            # Find path
            try:
                path = nx.shortest_path(self.causal_graph, cause, effect)
                # Effect diminishes with path length
                path_weight = 1.0
                for i in range(len(path) - 1):
                    if self.causal_graph.has_edge(path[i], path[i+1]):
                        edge_weight = self.causal_graph[path[i]][path[i+1]].get('weight', 0.5)
                        path_weight *= edge_weight
                return path_weight
            except:
                return 0.0
        else:
            return self.causal_graph[cause][effect].get('weight', 0.5)

class RuleGenerator:
    """
    Automatically generates symbolic rules from data and knowledge
    """

    def __init__(self, knowledge_base: SymbolicKnowledgeBase):
        self.kb = knowledge_base
        self.generated_rules = []

    def generate_rules_from_patterns(self, data: pd.DataFrame,
                                    min_confidence: float = 0.6) -> List[SymbolicRule]:
        """
        Generate rules from data patterns
        """
        rules = []

        # Pattern 1: Seasonal rules
        seasonal_rules = self._generate_seasonal_rules(data, min_confidence)
        rules.extend(seasonal_rules)

        # Pattern 2: Correlation rules
        correlation_rules = self._generate_correlation_rules(data, min_confidence)
        rules.extend(correlation_rules)

        # Pattern 3: Threshold rules
        threshold_rules = self._generate_threshold_rules(data, min_confidence)
        rules.extend(threshold_rules)

        # Add to knowledge base
        for rule in rules:
            self.kb.add_rule(rule)
            self.generated_rules.append(rule)

        return rules

    def _generate_seasonal_rules(self, data: pd.DataFrame, min_confidence: float) -> List[SymbolicRule]:
        """
        Generate rules based on seasonal patterns
        """
        rules = []

        if 'date' in data.columns and 'value' in data.columns:
            data['month'] = pd.to_datetime(data['date']).dt.month

            # Analyze monthly patterns
            monthly_avg = data.groupby('month')['value'].mean()
            overall_avg = data['value'].mean()

            for month, avg_value in monthly_avg.items():
                if avg_value > overall_avg * 1.2:
                    # High season
                    rule = SymbolicRule(
                        rule_id=f"seasonal_high_month_{month}",
                        rule_type=RuleType.CONDITIONAL,
                        condition=f"IF month is {month}",
                        conclusion=f"THEN demand increases",
                        confidence=min(0.8, (avg_value / overall_avg - 1)),
                        strength=(avg_value / overall_avg),
                        context={'month': month, 'pattern': 'seasonal_high'}
                    )
                    rules.append(rule)

                elif avg_value < overall_avg * 0.8:
                    # Low season
                    rule = SymbolicRule(
                        rule_id=f"seasonal_low_month_{month}",
                        rule_type=RuleType.CONDITIONAL,
                        condition=f"IF month is {month}",
                        conclusion=f"THEN demand decreases",
                        confidence=min(0.8, (1 - avg_value / overall_avg)),
                        strength=(overall_avg / avg_value),
                        context={'month': month, 'pattern': 'seasonal_low'}
                    )
                    rules.append(rule)

        return rules

    def _generate_correlation_rules(self, data: pd.DataFrame, min_confidence: float) -> List[SymbolicRule]:
        """
        Generate rules based on correlations
        """
        rules = []

        # Find numeric columns
        numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()

        for i, col1 in enumerate(numeric_cols):
            for col2 in numeric_cols[i+1:]:
                if col1 != col2:
                    try:
                        corr = data[col1].corr(data[col2])

                        if abs(corr) > 0.5:
                            if corr > 0:
                                rule = SymbolicRule(
                                    rule_id=f"corr_positive_{col1}_{col2}",
                                    rule_type=RuleType.CAUSAL,
                                    condition=f"IF {col1} increases",
                                    conclusion=f"THEN {col2} increases",
                                    confidence=min(0.9, abs(corr)),
                                    strength=abs(corr),
                                    context={'correlation': corr, 'variables': [col1, col2]}
                                )
                            else:
                                rule = SymbolicRule(
                                    rule_id=f"corr_negative_{col1}_{col2}",
                                    rule_type=RuleType.CAUSAL,
                                    condition=f"IF {col1} increases",
                                    conclusion=f"THEN {col2} decreases",
                                    confidence=min(0.9, abs(corr)),
                                    strength=abs(corr),
                                    context={'correlation': corr, 'variables': [col1, col2]}
                                )

                            if rule.confidence >= min_confidence:
                                rules.append(rule)
                    except Exception as e:
                        continue

        return rules

    def _generate_threshold_rules(self, data: pd.DataFrame, min_confidence: float) -> List[SymbolicRule]:
        """
        Generate rules based on threshold crossing
        """
        rules = []

        if 'value' in data.columns:
            mean_val = data['value'].mean()
            std_val = data['value'].std()

            high_threshold = mean_val + std_val
            low_threshold = mean_val - std_val

            # Analyze what happens after threshold crossing
            high_crosses = data[data['value'] > high_threshold]
            low_crosses = data[data['value'] < low_threshold]

            if len(high_crosses) > 5:
                rule = SymbolicRule(
                    rule_id="threshold_high_price",
                    rule_type=RuleType.CONDITIONAL,
                    condition=f"IF price exceeds {high_threshold:.2f}",
                    conclusion="THEN price likely to decrease (mean reversion)",
                    confidence=0.7,
                    strength=1.2,
                    context={'threshold': high_threshold, 'type': 'high'}
                )
                rules.append(rule)

            if len(low_crosses) > 5:
                rule = SymbolicRule(
                    rule_id="threshold_low_price",
                    rule_type=RuleType.CONDITIONAL,
                    condition=f"IF price falls below {low_threshold:.2f}",
                    conclusion="THEN price likely to increase (mean reversion)",
                    confidence=0.7,
                    strength=1.2,
                    context={'threshold': low_threshold, 'type': 'low'}
                )
                rules.append(rule)

        return rules

# Example usage and testing
def test_symbolic_reasoning():
    """Test the symbolic reasoning engine"""

    print("=== Testing Symbolic Reasoning Engine ===\n")

    # Initialize knowledge base
    kb = SymbolicKnowledgeBase()

    # Add sample rules
    rule1 = SymbolicRule(
        rule_id="diwali_onion_rule",
        rule_type=RuleType.CONDITIONAL,
        condition="IF diwali festival is approaching",
        conclusion="THEN onion demand increases",
        confidence=0.9,
        temporal_scope=(-7, 3),
        strength=3.0,
        context={'festival': 'diwali', 'product': 'onion'}
    )

    rule2 = SymbolicRule(
        rule_id="high_demand_price_rule",
        rule_type=RuleType.CAUSAL,
        condition="IF demand increases significantly",
        conclusion="THEN price increases",
        confidence=0.85,
        strength=1.5,
        context={'relationship': 'demand_price'}
    )

    rule3 = SymbolicRule(
        rule_id="monsoon_vegetable_rule",
        rule_type=RuleType.CONDITIONAL,
        condition="IF monsoon season with heavy rain",
        conclusion="THEN vegetable supply decreases",
        confidence=0.8,
        temporal_scope=(0, 30),
        strength=2.0,
        context={'season': 'monsoon', 'product_category': 'vegetables'}
    )

    kb.add_rule(rule1)
    kb.add_rule(rule2)
    kb.add_rule(rule3)

    print(f"Added {len(kb.rules)} rules to knowledge base\n")

    # Test rule retrieval
    context = {
        'entities': ['diwali', 'onion'],
        'festival_active': True,
        'days_offset': -5
    }

    applicable = kb.get_applicable_rules(context)
    print(f"Applicable rules for context: {len(applicable)}")
    for rule in applicable:
        print(f"  - {rule.rule_id}: {rule.condition} => {rule.conclusion}")

    # Test logical reasoning
    print("\n=== Testing Logical Inference ===")
    reasoner = LogicalReasoner(kb)

    query = "Will the price increase?"
    result = reasoner.infer(query, context)

    print(f"Query: {query}")
    print(f"Answer: {result['answer']}")
    print(f"Confidence: {result['confidence']:.2f}")
    print(f"Reasoning chain: {len(result['reasoning_chain'])} steps")

    for i, step in enumerate(result['reasoning_chain'], 1):
        print(f"  Step {i}: Applied {step['rule_id']}")
        print(f"    Inferred: {step['inferred_fact']}")

    # Test rule generation
    print("\n=== Testing Automatic Rule Generation ===")

    # Create sample data
    dates = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')
    sample_data = pd.DataFrame({
        'date': dates,
        'value': 50 + 10 * np.sin(2 * np.pi * np.arange(len(dates)) / 365) +
                np.random.normal(0, 3, len(dates))
    })

    rule_generator = RuleGenerator(kb)
    generated_rules = rule_generator.generate_rules_from_patterns(sample_data, min_confidence=0.6)

    print(f"Generated {len(generated_rules)} rules from data patterns:")
    for rule in generated_rules[:5]:  # Show first 5
        print(f"  - {rule.rule_id}")
        print(f"    {rule.condition} => {rule.conclusion}")
        print(f"    Confidence: {rule.confidence:.2f}\n")

    return kb, reasoner, rule_generator

if __name__ == "__main__":
    kb, reasoner, generator = test_symbolic_reasoning()

# Complete Integration of Neuro-Symbolic Physics-Informed Forecasting System

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
from datetime import datetime, timedelta
import json

# Import previous components (in practice, these would be imports)
# from part1_knowledge_graph import IndianMarketKnowledgeGraph
# from part2_physics_informed import MarketPhysicsEngine, MarketState
# from part3_symbolic_reasoning import SymbolicKnowledgeBase, LogicalReasoner, CausalInferenceEngine

@dataclass
class ForecastResult:
    """Complete forecast result with multi-modal explanations"""
    predictions: np.ndarray
    confidence_intervals: Dict[str, np.ndarray]
    certainty_score: float

    # Symbolic explanations
    symbolic_rules_applied: List[str]
    causal_chain: List[Dict[str, Any]]

    # Physics-based explanations
    physics_constraints: Dict[str, float]
    market_dynamics: Dict[str, Any]

    # Neural explanations
    attention_weights: Optional[np.ndarray]
    feature_importance: Dict[str, float]

    # Counterfactual scenarios
    what_if_scenarios: List[Dict[str, Any]]

    # Human-readable explanation
    narrative: str
    recommendations: List[str]

class MultiHeadAttention(nn.Module):
    """
    Multi-head attention mechanism for integrating different modalities
    """

    def __init__(self, embed_dim: int, num_heads: int = 8):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        assert self.head_dim * num_heads == embed_dim, "embed_dim must be divisible by num_heads"

        self.q_linear = nn.Linear(embed_dim, embed_dim)
        self.k_linear = nn.Linear(embed_dim, embed_dim)
        self.v_linear = nn.Linear(embed_dim, embed_dim)
        self.out_linear = nn.Linear(embed_dim, embed_dim)

    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)

        # Linear projections
        Q = self.q_linear(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.k_linear(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.v_linear(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)

        # Attention scores
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)

        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        attention_weights = F.softmax(scores, dim=-1)

        # Apply attention to values
        attended = torch.matmul(attention_weights, V)

        # Reshape and project
        attended = attended.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)
        output = self.out_linear(attended)

        return output, attention_weights

class NeuroSymbolicFusionLayer(nn.Module):
    """
    Fuses neural predictions with symbolic reasoning and physics constraints
    """

    def __init__(self, neural_dim: int, symbolic_dim: int, physics_dim: int, output_dim: int):
        super().__init__()

        self.neural_encoder = nn.Linear(neural_dim, 64)
        self.symbolic_encoder = nn.Linear(symbolic_dim, 64)
        self.physics_encoder = nn.Linear(physics_dim, 64)

        self.attention = MultiHeadAttention(64, num_heads=8)

        self.fusion_net = nn.Sequential(
            nn.Linear(64 * 3, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, output_dim)
        )

    def forward(self, neural_features, symbolic_features, physics_features):
        # Encode each modality
        neural_enc = F.relu(self.neural_encoder(neural_features))
        symbolic_enc = F.relu(self.symbolic_encoder(symbolic_features))
        physics_enc = F.relu(self.physics_encoder(physics_features))

        # Stack for attention
        stacked = torch.stack([neural_enc, symbolic_enc, physics_enc], dim=1)

        # Apply attention
        attended, attention_weights = self.attention(stacked, stacked, stacked)

        # Concatenate all information
        combined = torch.cat([
            attended[:, 0, :],  # Attended neural
            attended[:, 1, :],  # Attended symbolic
            attended[:, 2, :]   # Attended physics
        ], dim=-1)

        # Final fusion
        output = self.fusion_net(combined)

        return output, attention_weights

class NeuroSymbolicPhysicsForecaster:
    """
    Complete forecasting system integrating:
    - Knowledge Graphs (Symbolic)
    - Physics-Informed Neural ODEs
    - Symbolic Reasoning
    - Causal Inference
    - Multi-modal Attention
    """

    def __init__(self, product_categories: List[str]):
        # Initialize all components
        self.kg = IndianMarketKnowledgeGraph()
        self.physics_engine = MarketPhysicsEngine(self.kg)
        self.symbolic_kb = SymbolicKnowledgeBase()
        self.reasoner = LogicalReasoner(self.symbolic_kb)
        self.causal_engine = CausalInferenceEngine(self.kg)

        # Neural components
        self.fusion_layer = NeuroSymbolicFusionLayer(
            neural_dim=32,
            symbolic_dim=16,
            physics_dim=16,
            output_dim=1
        )

        self.product_categories = product_categories
        self.trained = False

    def train(self, historical_data: Dict[str, pd.DataFrame], epochs: int = 100):
        """
        Train the complete system
        """
        print("=" * 60)
        print("Training Neuro-Symbolic Physics-Informed Forecasting System")
        print("=" * 60)

        # Step 1: Train physics-informed model
        print("\n[1/4] Training Physics-Informed Neural ODEs...")
        physics_losses = self.physics_engine.train_physics_model(historical_data, epochs=epochs//2)
        print(f"Physics model trained. Final loss: {physics_losses[-1]:.4f}")

        # Step 2: Generate symbolic rules from data
        print("\n[2/4] Generating Symbolic Rules from Patterns...")
        from part3_symbolic_reasoning import RuleGenerator
        rule_generator = RuleGenerator(self.symbolic_kb)

        total_rules = 0
        for product, df in historical_data.items():
            rules = rule_generator.generate_rules_from_patterns(df, min_confidence=0.6)
            total_rules += len(rules)
        print(f"Generated {total_rules} symbolic rules")

        # Step 3: Discover causal relationships
        print("\n[3/4] Discovering Causal Relationships...")
        for product, df in historical_data.items():
            variables = df.select_dtypes(include=[np.number]).columns.tolist()
            if len(variables) > 1:
                causal_rels = self.causal_engine.discover_causal_relationships(df, variables)
                print(f"  {product}: Found {len(causal_rels)} causal relationships")

        # Step 4: Train fusion layer
        print("\n[4/4] Training Neuro-Symbolic Fusion Layer...")
        fusion_loss = self._train_fusion_layer(historical_data, epochs=epochs//2)
        print(f"Fusion layer trained. Final loss: {fusion_loss[-1]:.4f}")

        self.trained = True
        print("\n" + "=" * 60)
        print("System Training Completed Successfully!")
        print("=" * 60)

    def _train_fusion_layer(self, historical_data: Dict[str, pd.DataFrame], epochs: int = 50) -> List[float]:
        """
        Train the neuro-symbolic fusion layer
        """
        optimizer = torch.optim.Adam(self.fusion_layer.parameters(), lr=0.001)
        losses = []

        for epoch in range(epochs):
            epoch_loss = 0.0

            for product, df in historical_data.items():
                if len(df) < 20:
                    continue

                # Prepare features
                neural_features, symbolic_features, physics_features, targets = \
                    self._prepare_fusion_training_data(product, df)

                if neural_features is None:
                    continue

                # Forward pass
                predictions, attention_weights = self.fusion_layer(
                    neural_features, symbolic_features, physics_features
                )

                # Calculate loss
                loss = F.mse_loss(predictions.squeeze(), targets)

                # Backward pass
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                epoch_loss += loss.item()

            losses.append(epoch_loss)

            if epoch % 10 == 0:
                print(f"  Epoch {epoch}/{epochs}, Loss: {epoch_loss:.4f}")

        return losses

    def _prepare_fusion_training_data(self, product: str, df: pd.DataFrame) -> Tuple:
        """
        Prepare multi-modal training data
        """
        try:
            # Neural features: time series patterns
            values = df['value'].values if 'value' in df.columns else df.iloc[:, 1].values

            if len(values) < 20:
                return None, None, None, None

            # Create windows
            window_size = 10
            neural_features_list = []
            targets_list = []

            for i in range(window_size, len(values)):
                window = values[i-window_size:i]

                # Statistical features
                neural_feat = np.array([
                    window.mean(),
                    window.std(),
                    window[-1] - window[0],  # Trend
                    np.percentile(window, 75) - np.percentile(window, 25),  # IQR
                    *window[-3:]  # Last 3 values
                ])

                # Pad to fixed size (32 dimensions)
                neural_feat = np.pad(neural_feat, (0, max(0, 32 - len(neural_feat))))[:32]

                neural_features_list.append(neural_feat)
                targets_list.append(values[i])

            # Symbolic features: rules and knowledge
            symbolic_features_list = []
            for _ in range(len(neural_features_list)):
                symbolic_feat = self._extract_symbolic_features(product)
                symbolic_features_list.append(symbolic_feat)

            # Physics features: market state
            physics_features_list = []
            for _ in range(len(neural_features_list)):
                physics_feat = self._extract_physics_features(product, values)
                physics_features_list.append(physics_feat)

            # Convert to tensors
            neural_features = torch.FloatTensor(np.array(neural_features_list))
            symbolic_features = torch.FloatTensor(np.array(symbolic_features_list))
            physics_features = torch.FloatTensor(np.array(physics_features_list))
            targets = torch.FloatTensor(np.array(targets_list))

            return neural_features, symbolic_features, physics_features, targets

        except Exception as e:
            print(f"Error preparing fusion data for {product}: {e}")
            return None, None, None, None

    def _extract_symbolic_features(self, product: str) -> np.ndarray:
        """
        Extract symbolic features from knowledge graph and rules
        """
        features = np.zeros(16)

        # Get related entities and their weights
        related = self.kg.get_related_entities(product, max_hops=1)

        for i, (entity, weight) in enumerate(related[:8]):
            features[i] = weight

        # Count applicable rules
        context = {'entities': [product], 'product': product}
        applicable_rules = self.symbolic_kb.get_applicable_rules(context)
        features[8] = len(applicable_rules)

        # Average rule confidence
        if applicable_rules:
            features[9] = np.mean([r.confidence for r in applicable_rules])
            features[10] = np.mean([r.strength for r in applicable_rules])

        # Entity type encoding
        if product in self.kg.graph:
            entity_type = self.kg.graph.nodes[product].get('entity_type')
            if entity_type:
                features[11] = hash(entity_type.value) % 10 / 10.0

        return features

    def _extract_physics_features(self, product: str, values: np.ndarray) -> np.ndarray:
        """
        Extract physics-based features
        """
        features = np.zeros(16)

        if len(values) > 5:
            # Market dynamics features
            features[0] = values[-1]  # Current price
            features[1] = np.mean(values[-5:])  # Recent average
            features[2] = np.std(values[-5:])  # Volatility
            features[3] = values[-1] - values[-5]  # Momentum

            # Supply-demand proxy (from price changes)
            price_changes = np.diff(values[-10:])
            features[4] = np.mean(price_changes > 0)  # Demand strength
            features[5] = np.mean(price_changes < 0)  # Supply strength

            # Energy-like features
            features[6] = 0.5 * (features[0]**2 + features[3]**2)  # Market energy

            # Conservation proxy
            features[7] = np.sum(price_changes)  # Net flow

        return features

    def forecast(self, product: str, historical_data: pd.DataFrame,
                forecast_horizon: int = 30, generate_scenarios: bool = True) -> ForecastResult:
        """
        Generate comprehensive forecast with multi-modal explanations
        """
        if not self.trained:
            raise ValueError("System must be trained first!")

        print(f"\n{'='*60}")
        print(f"Generating Forecast for: {product}")
        print(f"Horizon: {forecast_horizon} days")
        print(f"{'='*60}\n")

        # Step 1: Physics-based forecast
        print("[1/5] Physics-Informed Dynamics...")
        initial_state = self._create_initial_state(historical_data)
        physics_forecast = self.physics_engine.forecast_market_dynamics(
            product, initial_state, forecast_horizon
        )
        physics_predictions = np.array([s.price for s in physics_forecast])
        print(f"  Physics forecast: {physics_predictions[0]:.2f}  {physics_predictions[-1]:.2f}")

        # Step 2: Symbolic reasoning
        print("[2/5] Symbolic Reasoning...")
        symbolic_context = self._build_symbolic_context(product, historical_data)
        symbolic_inference = self.reasoner.infer(
            "What will happen to the price?",
            symbolic_context
        )
        symbolic_rules_applied = symbolic_inference['applicable_rules']
        print(f"  Applied {len(symbolic_rules_applied)} symbolic rules")

        # Step 3: Neural-Symbolic Fusion
        print("[3/5] Neuro-Symbolic Fusion...")
        fusion_predictions = self._neural_symbolic_prediction(
            product, historical_data, forecast_horizon
        )
        print(f"  Fusion forecast: {fusion_predictions[0]:.2f}  {fusion_predictions[-1]:.2f}")

        # Step 4: Ensemble and Uncertainty Quantification
        print("[4/5] Ensemble Integration...")
        final_predictions, confidence_intervals = self._ensemble_predictions(
            physics_predictions, fusion_predictions
        )
        certainty_score = self._calculate_certainty(
            physics_predictions, fusion_predictions, historical_data
        )
        print(f"  Certainty score: {certainty_score:.2%}")

        # Step 5: Generate Explanations
        print("[5/5] Generating Explanations...")

        # Causal analysis
        causal_chain = self._build_causal_explanation(product, symbolic_context)

        # Physics constraints
        physics_constraints = self._extract_physics_constraints(physics_forecast)

        # Market dynamics summary
        market_dynamics = self.physics_engine.analyze_market_stability(product, physics_forecast)

        # Feature importance
        feature_importance = self._compute_feature_importance(historical_data)

        # Counterfactual scenarios
        what_if_scenarios = []
        if generate_scenarios:
            what_if_scenarios = self._generate_counterfactual_scenarios(
                product, historical_data, symbolic_context
            )

        # Generate narrative
        narrative = self._generate_narrative(
            product, final_predictions, symbolic_rules_applied,
            causal_chain, market_dynamics, certainty_score
        )

        # Generate recommendations
        recommendations = self._generate_recommendations(
            product, final_predictions, market_dynamics, what_if_scenarios
        )

        print(f"\n{'='*60}")
        print("Forecast Complete!")
        print(f"{'='*60}\n")

        return ForecastResult(
            predictions=final_predictions,
            confidence_intervals=confidence_intervals,
            certainty_score=certainty_score,
            symbolic_rules_applied=symbolic_rules_applied,
            causal_chain=causal_chain,
            physics_constraints=physics_constraints,
            market_dynamics=market_dynamics,
            attention_weights=None,
            feature_importance=feature_importance,
            what_if_scenarios=what_if_scenarios,
            narrative=narrative,
            recommendations=recommendations
        )

    def _create_initial_state(self, df: pd.DataFrame) -> 'MarketState':
        """Create initial market state from historical data"""
        from part2_physics_informed import MarketState

        values = df['value'].values if 'value' in df.columns else df.iloc[:, 1].values

        return MarketState(
            price=float(values[-1]),
            supply=100.0,
            demand=100.0,
            inventory=200.0,
            momentum=float(values[-1] - values[-2]) if len(values) > 1 else 0.0,
            volatility=float(np.std(values[-7:])) if len(values) > 7 else 1.0,
            timestamp=datetime.now()
        )

    def _build_symbolic_context(self, product: str, df: pd.DataFrame) -> Dict[str, Any]:
        """Build context for symbolic reasoning"""
        values = df['value'].values if 'value' in df.columns else df.iloc[:, 1].values

        return {
            'product': product,
            'entities': [product],
            'current_price': float(values[-1]),
            'price_trend': 'increasing' if values[-1] > values[-5] else 'decreasing',
            'volatility_level': 'high' if np.std(values[-7:]) > np.std(values) else 'low',
            'demand_level': 'high' if values[-1] > np.mean(values) else 'normal',
            'festival_active': False,  # Would be determined by calendar
            'days_offset': 0
        }

    def _neural_symbolic_prediction(self, product: str, df: pd.DataFrame,
                                   horizon: int) -> np.ndarray:
        """Generate predictions using fusion layer"""
        values = df['value'].values if 'value' in df.columns else df.iloc[:, 1].values

        predictions = []
        current_values = values.copy()

        for _ in range(horizon):
            # Prepare features
            neural_feat = self._extract_symbolic_features(product)
            neural_feat = np.pad(neural_feat, (0, 32 - len(neural_feat)))[:32]

            symbolic_feat = self._extract_symbolic_features(product)
            physics_feat = self._extract_physics_features(product, current_values)

            # Convert to tensors
            neural_tensor = torch.FloatTensor(neural_feat).unsqueeze(0)
            symbolic_tensor = torch.FloatTensor(symbolic_feat).unsqueeze(0)
            physics_tensor = torch.FloatTensor(physics_feat).unsqueeze(0)

            # Predict
            with torch.no_grad():
                pred, _ = self.fusion_layer(neural_tensor, symbolic_tensor, physics_tensor)
                next_value = pred.item()

            predictions.append(next_value)
            current_values = np.append(current_values, next_value)

        return np.array(predictions)

    def _ensemble_predictions(self, physics_preds: np.ndarray,
                            fusion_preds: np.ndarray) -> Tuple[np.ndarray, Dict]:
        """Ensemble predictions with uncertainty quantification"""
        # Weighted average (physics-informed gets higher weight)
        weights = [0.6, 0.4]  # Physics, Fusion

        final_preds = weights[0] * physics_preds + weights[1] * fusion_preds

        # Calculate confidence intervals
        std_dev = np.std([physics_preds, fusion_preds], axis=0)

        confidence_intervals = {
            '80%': np.column_stack([
                final_preds - 1.28 * std_dev,
                final_preds + 1.28 * std_dev
            ]),
            '95%': np.column_stack([
                final_preds - 1.96 * std_dev,
                final_preds + 1.96 * std_dev
            ])
        }

        return final_preds, confidence_intervals

    def _calculate_certainty(self, physics_preds: np.ndarray,
                           fusion_preds: np.ndarray, df: pd.DataFrame) -> float:
        """Calculate overall certainty score"""
        # Agreement between models
        agreement = 1 - np.mean(np.abs(physics_preds - fusion_preds) / physics_preds)

        # Historical data quality
        values = df['value'].values if 'value' in df.columns else df.iloc[:, 1].values
        data_quality = min(1.0, len(values) / 365)  # More data = better quality

        # Volatility penalty
        volatility = np.std(values) / np.mean(values)
        volatility_score = 1 / (1 + volatility)

        certainty = (agreement * 0.5 + data_quality * 0.3 + volatility_score * 0.2)
        return max(0.0, min(1.0, certainty))

    def _build_causal_explanation(self, product: str, context: Dict) -> List[Dict[str, Any]]:
        """Build causal explanation chain"""
        causal_chain = []

        # Get related entities
        related = self.kg.get_related_entities(product, max_hops=2)

        for entity, weight in related[:5]:
            if self.kg.causal_graph.has_edge(entity, product):
                causal_chain.append({
                    'cause': entity,
                    'effect': product,
                    'strength': weight,
                    'mechanism': f"{entity} influences {product} through market dynamics"
                })

        return causal_chain

    def _extract_physics_constraints(self, forecast: List) -> Dict[str, float]:
        """Extract physics constraints from forecast"""
        prices = [s.price for s in forecast]
        supplies = [s.supply for s in forecast]
        demands = [s.demand for s in forecast]

        return {
            'supply_demand_balance': np.mean([abs(s - d) / (s + d + 1e-6)
                                            for s, d in zip(supplies, demands)]),
            'price_momentum_conservation': np.std([s.momentum for s in forecast]),
            'energy_conservation': np.std([0.5 * (s.price**2 + s.momentum**2) for s in forecast]),
            'volatility_trend': np.corrcoef(range(len(forecast)),
                                          [s.volatility for s in forecast])[0, 1]
        }

    def _compute_feature_importance(self, df: pd.DataFrame) -> Dict[str, float]:
        """Compute feature importance scores"""
        importance = {}

        if 'value' in df.columns:
            values = df['value'].values

            importance['recent_trend'] = abs(np.corrcoef(range(len(values[-30:])), values[-30:])[0, 1])
            importance['seasonality'] = abs(np.corrcoef(range(len(values)),
                                           np.sin(2 * np.pi * np.arange(len(values)) / 365))[0, 1])
            importance['volatility'] = np.std(values) / np.mean(values)

        return importance

    def _generate_counterfactual_scenarios(self, product: str, df: pd.DataFrame,
                                          context: Dict) -> List[Dict[str, Any]]:
        """Generate what-if scenarios"""
        scenarios = []

        # Scenario 1: What if there was a festival?
        festival_scenario = {
            'name': 'Festival Impact',
            'description': f'What if a major festival occurred during the forecast period?',
            'expected_change': '+20% to +50% increase in demand',
            'confidence': 0.8
        }
        scenarios.append(festival_scenario)

        # Scenario 2: What if supply disruption?
        supply_scenario = {
            'name': 'Supply Disruption',
            'description': f'What if there was a 30% supply reduction?',
            'expected_change': '+40% to +60% price increase',
            'confidence': 0.75
        }
        scenarios.append(supply_scenario)

        # Scenario 3: What if weather impact?
        weather_scenario = {
            'name': 'Adverse Weather',
            'description': f'What if monsoon affected the region?',
            'expected_change': '-20% to -40% supply decrease, +30% price increase',
            'confidence': 0.7
        }
        scenarios.append(weather_scenario)

        return scenarios

    def _generate_narrative(self, product: str, predictions: np.ndarray,
                          rules_applied: List[str], causal_chain: List[Dict],
                          market_dynamics: Dict, certainty: float) -> str:
        """Generate human-readable narrative explanation"""

        price_change = ((predictions[-1] / predictions[0]) - 1) * 100
        direction = "increase" if price_change > 0 else "decrease"

        narrative = f"**Forecast Analysis for {product.title()}**\n\n"

        narrative += f"**Overall Prediction**: The price is expected to {direction} by {abs(price_change):.1f}% "
        narrative += f"over the next {len(predictions)} days (Certainty: {certainty:.0%}).\n\n"

        narrative += f"**Key Drivers**:\n"

        # Causal factors
        if causal_chain:
            narrative += f"- **Causal Factors**: "
            top_causes = causal_chain[:3]
            causes_text = ", ".join([f"{c['cause']}" for c in top_causes])
            narrative += f"{causes_text} are influencing the market.\n"

        # Market phase
        market_phase = market_dynamics.get('market_phase', 'Unknown')
        narrative += f"- **Market Phase**: Currently in {market_phase} phase.\n"

        # Stability
        stability = market_dynamics.get('stability_score', 0.5)
        stability_level = "High" if stability > 0.7 else "Moderate" if stability > 0.4 else "Low"
        narrative += f"- **Market Stability**: {stability_level} ({stability:.0%})\n"

        # Supply-demand balance
        balance = market_dynamics.get('supply_demand_balance', 0.5)
        if balance > 0.7:
            narrative += f"- **Supply-Demand**: Well balanced, supporting price stability.\n"
        else:
            narrative += f"- **Supply-Demand**: Imbalanced, contributing to volatility.\n"

        narrative += f"\n**Applied Knowledge**: {len(rules_applied)} symbolic rules and "
        narrative += f"{len(causal_chain)} causal relationships were considered in this forecast.\n"

        return narrative

    def _generate_recommendations(self, product: str, predictions: np.ndarray,
                                 market_dynamics: Dict, scenarios: List[Dict]) -> List[str]:
        """Generate actionable recommendations"""
        recommendations = []

        price_trend = predictions[-1] - predictions[0]

        if price_trend > 0:
            recommendations.append(f" Consider stocking up on {product} before prices rise further")
            recommendations.append(f" Monitor supply chain for potential disruptions")
        else:
            recommendations.append(f" Delay purchases if possible to benefit from lower prices")
            recommendations.append(f" Review inventory levels to avoid overstocking")

        # Stability-based recommendations
        stability = market_dynamics.get('stability_score', 0.5)
        if stability < 0.5:
            recommendations.append(f" High volatility expected - maintain buffer inventory")
            recommendations.append(f" Consider hedging strategies for price protection")

        # Scenario-based recommendations
        if scenarios:
            recommendations.append(f" Prepare contingency plans for {len(scenarios)} identified risk scenarios")

        return recommendations

# Example usage and comprehensive testing
def comprehensive_system_test():
    """
    Comprehensive test of the complete neuro-symbolic system
    """
    print("\n" + "="*70)
    print(" NEURO-SYMBOLIC PHYSICS-INFORMED FORECASTING SYSTEM")
    print(" Complete Integration Test")
    print("="*70 + "\n")

    # Initialize system
    products = ['onions', 'rice', 'wheat']
    forecaster = NeuroSymbolicPhysicsForecaster(products)

    # Create sample data
    dates = pd.date_range(start='2023-01-01', end='2024-03-01', freq='D')

    historical_data = {
        'onions': pd.DataFrame({
            'date': dates,
            'value': 50 + 10 * np.sin(2 * np.pi * np.arange(len(dates)) / 365) +
                    5 * np.sin(2 * np.pi * np.arange(len(dates)) / 7) +
                    np.random.normal(0, 3, len(dates))
        }),
        'rice': pd.DataFrame({
            'date': dates,
            'value': 30 + 5 * np.sin(2 * np.pi * np.arange(len(dates)) / 365) +
                    np.random.normal(0, 2, len(dates))
        }),
    }

    # Train system
    forecaster.train(historical_data, epochs=20)

    # Generate forecast
    result = forecaster.forecast('onions', historical_data['onions'], forecast_horizon=30)

    # Display results
    print("\n" + "="*70)
    print(" FORECAST RESULTS")
    print("="*70 + "\n")

    print(result.narrative)

    print(f"\n**Confidence Intervals (95%)**:")
    print(f"Day 1: [{result.confidence_intervals['95%'][0, 0]:.2f}, {result.confidence_intervals['95%'][0, 1]:.2f}]")
    print(f"Day 30: [{result.confidence_intervals['95%'][-1, 0]:.2f}, {result.confidence_intervals['95%'][-1, 1]:.2f}]")

    print(f"\n**Recommendations**:")
    for i, rec in enumerate(result.recommendations, 1):
        print(f"{i}. {rec}")

    print(f"\n**What-If Scenarios**:")
    for scenario in result.what_if_scenarios:
        print(f"\n- {scenario['name']}")
        print(f"  {scenario['description']}")
        print(f"  Expected: {scenario['expected_change']}")
        print(f"  Confidence: {scenario['confidence']:.0%}")

    return forecaster, result

if __name__ == "__main__":
    forecaster, result = comprehensive_system_test()